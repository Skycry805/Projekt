{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nikla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, TrainingArguments, Trainer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten laden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('encoded-Test.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prüfen ob null ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Idiom ID        0\n",
       "Idiom           0\n",
       "German Idiom    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrahieren von Features und Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ein Fisch auf dem Trockenen', 'ein Klacks sein', 'Öl ins Feuer gießen', 'gegen den Strich gehen', 'bis zu den Zähnen bewaffnet sein', 'sich gleichen wie ein Ei dem anderen', 'in jmds Hinterkopf sein', 'wieder so weit sein wie zuvor', 'die Buchführung abstimmen', 'die Trommel rühren', 'ein Spielverderber sein', 'auf Wolke sieben schweben', 'jdm wie aus dem Gesicht geschnitten sein', 'jdm. zuvorkommen', 'hinter dem Mond leben', 'unter der Gürtellinie', 'Gift auf etwas nehmen', 'Luftschlösser bauen', 'jmd. den Kopf abreißen', 'in den sauren Apfel beißen', 'ins Gras beißen', 'den Weg freimachen', 'eine Sicherung durchbrennen', 'jmd. umhauen', 'jmd ins Jenseits befördern', 'an die Decke gehen', 'mit einem silbernen Löffel im Mund geboren', 'die Bank sprengen', 'das Eis brechen', 'den Rekord brechen', 'die Brötchen verdienen', 'eine Nachtschicht einlegen', 'das Kriegsbeil begraben', 'sich einen abbrechen', 'durch mündliche Überlieferung', 'eine Versammlung einberufen', 'den Ton angeben', 'seinen Ohren nicht trauen', 'nach jdm. schmachten', 'den Kopf hinhalten', 'Aufsehen erregen', 'Aufsehen erregen', 'über Gott und die Welt reden', 'reinen Tisch machen', 'klar Schiff machen zum Gefecht', 'auf den fahrenden Zug aufspringen', 'hibbelig sein', 'sich zum Besten wenden', 'jdm den Garaus machen', 'die Bücher frisieren', 'sich die Beine in den Bauch stehen', 'schweineteuer sein', 'dafür büßen müssen / teuer dafür bezahlen müssen', 'ein weites Gebiet abdecken', 'seine Spuren verwischen', 'eine Flasche köpfen', 'einen Witz reißen', 'ein Lächeln zustande bringen', 'die Peitsche schwingen', 'jemandem im Weg sein', 'nach jmds Pfeife tanzen', 'seine Versprechungen einlösen', 'sich die Ehre geben', 'die Runde machen', 'den Zweck erfüllen', 'eine Niete ziehen', 'die Grenze ziehen', 'aufgedonnert sein', 'hart verhandeln', 'eine Bombe platzen lassen', 'zurücknehmen was man gesagt hat', 'die Augen sind größer als der Magen', 'die Suppe auslöffeln', 'in Ungnade fallen', 'auf taube Ohren stoßen', 'Öl ins Feuer gießen', 'seine Schäfchen ins Trockene bringen', 'fit wie ein Turnschuh', 'den Anforderungen entsprechen', 'eine Eintagsfliege', 'einen Versuchsballon steigen lassen', 'sich aus dem Staub machen', 'mit der Masse gehen', 'Denkanstoß', 'die Zeche bezahlen', 'jdm keine andere Wahl lassen', 'jdm zu Tode erschrecken', 'einen Frosch im Hals haben', 'sich am Riemen reißen', 'den Stein ins Rollen bringen', 'den Moralischen haben', 'einen Korb bekommen', 'das Rederecht haben', 'eine Nachricht richtig verstehen', 'grünes Licht haben', 'kapieren', 'rausgeschmissen werden', 'jmd auf die Palme bringen', 'mit dem falschen Fuß aufstehen', 'des Guten zu viel tun', 'Gänsehaut bekommen', 'Zustände kriegen', 'einen Versuch wagen', 'jmd freie Hand lassen', 'jdm die kalte Schulter zeigen', 'bis zum Äußersten gehen', 'aufs Ganze gehen', 'aus den Fugen geraten', 'den Weg ebnen', 'mit Samthandschuhen anfassen', 'Spaß haben', 'einen Koller bekommen', 'einen Versuch unternehmen', 'eine Schraube locker haben', 'einen Versuch wagen', 'sein eigenes Süppchen kochen', 'eine Ausrede parat haben', 'kalte Füße bekommen', 'dumm dastehen', 'das Herz haben (etw übers Herz bringen)', 'auf Schwierigkeiten stoßen', 'die Nase in die Bücher stecken', 'zur Flasche greifen', 'an die Decke gehen', 'in die Schlagzeilen kommen', 'den Jackpot knacken', 'sich auf den Weg machen', 'an die Decke gehen', 'sich aufs Ohr hauen', 'sich volllaufen lassen', 'gut schmecken ', 'per Anhalter fahren', 'die Trümpfe in der Hand halten', 'die Stellung halten', 'immer mit der Ruhe', 'ein anderes Paar Schuhe', 'in der Patsche sitzen', 'in der Klemme stecken', 'im siebten Himmel', 'in letzter Sekunde', 'einen Fehlstart hinlegen', 'einen kühlen Kopf bewahren', 'ein Ass im Ärmel haben', 'eine Gewohnheit ablegen', 'den Löffel abgeben', 'auf den Putz hauen', 'wissen wie der Hase läuft', 'ein Machtwort sprechen', 'zu nichts führen', 'auswendig lernen', 'lernen wie die Dinge laufen', 'jdn auf dem Trockenen sitzen lassen', 'jdm sein Ohr leihen', 'mit anpacken', 'die Katze aus dem Sack lassen', 'nach den Buchstaben des Gesetzes', 'nach Strich und Faden lügen', 'einen Finger rühren', 'eine Lüge leben', 'der lange Arm des Gesetzes', 'für die Rolle passen / glaubwürdig wirken', 'Gesicht verlieren', 'nachlassen', 'den Faden verlieren', 'die Fassung verlieren', 'den Halt verlieren', 'Tabula rasa machen', 'einen Aufstand machen', 'einen großen Reibach machen', 'seinen Lebensunterhalt verdienen', 'Anstalten machen', 'jmd an die Wäsche gehen', 'eine schöne Stange Geld machen', 'Furore machen', 'keinen Hehl aus etw machen', 'den Anforderungen gerecht werden', 'eine Szene machen', 'es schaffen', 'den Anschluss verpassen', 'am Ziel vorbeischießen', 'Verwirrung stiften', 'das Datum nennen', 'etw im Keim ersticken', 'gegen jdn einen Groll hegen', 'die Dinge erleichtern', 'aus dem Blauen heraus', 'aus heiterem Himmel', 'auf dem absteigenden Ast sein', 'den Bogen überspannen', 'es in sich haben', 'zu viel Geld verlangen', 'die Sau rauslassen', 'nichts anders zu erwarten', 'den schwarzen Peter weitergeben', 'den Hut herumreichen', 'den Weg ebnen', 'ein Lippenbekenntnis ablegen', 'die Zeche bezahlen', 'Wucherpreise bezahlen', 'eine Rolle spielen', 'es darauf ankommen lassen', 'eine Beziehung nach der anderen haben', 'sich zum Narren machen', 'sich an die Regeln halten', 'an der Börse spekulieren', 'mit dem Feuer spielen', 'einen Heiratsantrag machen', 'die Straßen abklappern', 'jmd sein Herz ausschütten', 'in den Himmel loben', 'jdm auf den Arm nehmen', 'jdm den Stecker ziehen', 'jdm den Boden unter den Füßen wegziehen', 'die Daumenschrauben anlegen', 'sich den Kopf zerbrechen', 'es gießt wie aus Kübeln', 'eine Augenbraue hochziehen', 'an die Decke gehen', 'alte Geschichten ausgraben', 'zwischen den Zeilen lesen', 'das Kompliment erwidern', 'dem Sturm standhalten', 'jmd an etw erinnern', 'anbeißen', 'mit jemandem ausgehen der/die viel jünger ist', 'Staub aufwirbeln', 'Faustregel', 'Herr im Hause sein', 'mit eisener Faust regieren', 'es ist zum Davonlaufen', 'etw zugrunde richten', 'Spießruten laufen', 'die Lage retten', 'seine Haut retten', 'etwas Wichtiges sagen', 'ein Wort genügt', 'eine Hand wäscht die andere', 'an der Oberfläche kratzen', 'brüllen wie am Spieß', 'jmds  Schicksal besiegeln', 'zur Erkenntnis kommen', 'Erfahrung sammeln', 'die Gelegenheit am Schopf packen/nutzen', 'den Takt vorgeben', 'den Rahmen abstecken', 'ein Hühnchen rupfen', 'einen Plausch halten', 'Flagge zeigen', 'die Klappe halten', 'zwischen den Fronten stehen', 'sich auf dünnem Eis bewegen', 'jdm entfallen', 'Lunte riechen', 'frei von der Leber weg sprechen', 'alles ausplaudern', 'Seemannsgarn spinnen', 'Haare spalten', 'sich auf halbem Weg entgegenkommen', 'eine Chance haben', 'die Sache durchziehen', 'jdm die Schau stehlen', 'Kopf und Kragen riskieren', 'Blut in Wallung bringen', 'übertreiben', 'ein gutes Geschäft abschließen', 'auf Anklang stoßen', 'die Pille versüßen', 'seinen Stolz schlucken', 'alles abräumen', 'sich drücken', 'hinfallen', 'jdn über´s Ohr hauen', 'die zweite Geige spielen', 'den Köder schlucken', 'den Stier an den Hörnern packen', 'den Vogel abschießen', 'den Sprung wagen', 'als Zeuge auftreten', 'mit Vorsicht genießen', 'ohne Punkt und Komma reden', 'Geld aus dem Fenster werfen', 'in die Tasten hauen', 'den Bund der Ehe schließen', 'Spitze des Eisbergs', 'den Ausschlag geben', 'den Ausschlag geben', 'spuren gehorchen', 'es geht aufwaerts', 'den Spieß umdrehen', 'das Blatt wenden', 'jmd unter Druck setzen', 'unter jds Fuchtel stehen', 'angeschlagen sein', 'den Einsatz erhöhen', 'einen Drahtseilakt vollführen', 'in den Wind reden', 'die Hosen anhaben', 'eine Sache überstehen', 'mit Glanz und Gloria', 'alles für etw geben']\n",
      "['a fish out of water', 'a piece of cake', 'add fuel to the fire', 'go against the grain', 'armed to the teeth', 'like two peas in a pod', 'be at the back of ones mind', 'be back to square one', 'balance the books', 'bang/beat the drum', 'be a wet blanket', 'be on cloud nine', 'be the spitting image', 'beat to the punch', 'behind the times', 'below the belt', 'bet your bottom dollar', 'build castles in the air', 'bite someones head off', 'bite the bullet', 'bite the dust', 'blaze the trail', 'blow a fuse', 'blow someones mind', 'blow to kingdome come', 'blow your top', 'born with a silver spoon', 'break the bank', 'break the ice', 'break the record', 'bring home the bacon', 'burn the midnight oil', 'bury the hatchet', 'bust a gut', 'by word of mouth', 'call a meeting', 'call the shots', 'cant believe my ears', 'carry a torch (for someone)', 'carry the can', 'cause a stir', 'cause an uproar', 'chew the fat', 'clear the air', 'clear the decks', 'climb (jump on/get on) on the band wagon', 'climb (up) the walls', 'come up roses', 'cook his goose', 'cook the books', 'cool his heels', 'cost an arm and a leg', 'count the cost', 'cover the territory', 'cover up ones tracks', 'crack open a bottle', 'crack a joke', 'crack a smile', 'crack the whip', 'cramp someones style', 'dance to another tune', 'deliver the goods', 'do the honors', 'do the rounds', 'do the trick', 'draw a blank', 'draw the line', 'dressed to kill', 'drive a hard bargain', 'drop a bombshell', 'eat his words', 'ones eyes are bigger than ones stomach', 'face the music', 'fall from grace', 'fall on deaf ears', 'fan the flames', 'feather ones nest', 'fit as a fiddle', 'fit the bill', '(+a) flash in the pan', 'fly a kite', 'fly the coop', 'follow the crowd', 'food for thought', 'foot the bill', 'force someones hand', 'frighten out of ones wits', 'have a frog in ones throat', 'get a grip', 'get the ball rolling', 'get/have the blues', 'get the brushoff', 'get the floor', 'get the message', 'get the nod', 'get the picture', 'get the sack', 'get your goat', 'get up on the wrong side of the bed', 'gild the lily', 'give (someone) the creeps', 'give (someone) the willies', 'give it a whirl', 'give plenty of rope', 'give the cold shoulder', 'go the limit', 'go the whole hog', 'go to pieces', 'grease the wheels', 'handle with kid gloves', 'have a ball', 'have a fit', 'have a go', 'have a screw loose', 'have/take a stab', 'have an ax to grind', 'have an out', 'have cold feet', 'have (an) egg on your face', 'have the heart', 'hit a snag', 'hit the books', 'hit the bottle', 'hit the ceiling', 'hit the headlines', 'hit the jackpot', 'hit the road', 'hit the roof', 'hit the sack', 'hit the sauce', 'hit the spot', 'hitch a ride', 'hold the aces', 'hold (down) the fort', 'hold your horses', 'horse of another color', 'in a pickle', 'in hot water', 'in seventh heaven', 'in the nick of time', 'jump the gun', 'keep a level head', 'keep an ace up your sleeve', 'kick a habit', 'kick the bucket', 'kick up your heels', 'know the score', 'lay down the law', 'lead up a blind alley', 'learn by heart', 'learn the ropes', 'leave someone high and dry', 'lend (someone) an ear', 'lend a hand', 'let the cat out of the bag', 'letter of the law', 'lie through ones teeth', 'lift a finger', 'live a lie', 'long arm of the law', 'look the part', 'lose face', 'lose ones touch', 'lose the thread', 'lose your cool', 'lose your grip', 'make a clean sweep', 'make a fuss', 'make a killing', 'make a living', 'make a move', 'make a pass', 'make a pile', 'make a splash', 'make no bones about (it)', 'make the grade', 'make a scene', 'make the scene', 'miss the boat', 'miss the mark', 'muddy the waters', 'name the day (date)', 'nip in the bud', 'nurse a grudge', 'oil the wheels', 'out of the blue', 'out of thin air', 'over the hill', 'overstep the mark', 'pack a punch', 'pad the bill', 'paint the town', 'par for the course', 'pass the buck', 'pass the hat', 'pave the way', 'pay lip service', 'pay the piper', 'pay through the nose', 'play a part', 'play by ear', 'play the field', 'play the fool', 'play the game', 'play the market', 'play with fire', 'pop the question', 'pound the pavement', 'pour ones heart out', 'praise to the skies', 'pull someones leg', 'pull the plug (on something or someone)', 'pull the rug out from under someones feet', 'put the screws on', 'rack ones brains', 'its raining cats and dogs', 'raise an eyebrow', 'raise the roof', 'rake over the coals', 'read between the lines', 'return the compliment', 'ride the storm', 'ring a bell', 'rise to the bait', 'rob the cradle', 'rock the boat', 'rule of thumb', 'rule the roost', 'rule with an iron fist', 'run a mile', 'run into the ground', 'run the gauntlet', 'save the day', 'save your skin', 'say a mouthful', 'say the word', 'you scratch my back Ill scratch yours', 'scratch the surface', 'scream bloody murder', 'seal ones fate', 'see the light', 'see the world', 'seize the opportunity', 'set the pace', 'set the scene', 'settle a score', 'shoot the breeze', 'show the flag', 'shut your trap', 'sit on the fence', 'skate on thin ice', 'slip ones mind', 'smell a rat', 'speak your mind', 'spill the beans', 'spin a yarn', 'split hairs', 'split the difference', 'stand a chance', 'stay the course', 'steal someones thunder', 'to stick ones neck out', 'stir the blood', 'stretch a point', 'strike a bargain', 'strike a chord', 'sugar the pill', 'swallow ones pride', 'sweep the board', 'swing the lead', 'take a spill', 'take someone to the cleaners', 'take the back seat', 'take the bait', 'take the bull by the horns', 'take the cake', 'take the plunge', 'take the stand', 'take with a grain of salt', 'talk a mile a minute', 'throw money out the window', 'tickle the ivories', 'tie the knot', 'tip of the iceberg', 'tip the balance', 'tip the scales', 'toe the line', 'turn the corner', 'turn the tables', 'turn the tide', 'twist someones arm', 'under someones thumb', 'under the weather', 'up the ante', 'walk a tightrope', 'waste your breath', 'wear the pants', 'weather the storm', 'with flying colors', 'would give the world']\n"
     ]
    }
   ],
   "source": [
    "features = train_data['German Idiom'].tolist()\n",
    "labels = train_data['Idiom'].tolist()\n",
    "\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vortrainiertes Model aufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #Berechnung auf GPU verschieben\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels und feature tokiniesieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   53,  6022,    37,    57, 14229,    15,     0, 58100, 58100, 58100,\n",
      "         58100, 58100],\n",
      "        [   53,   272, 13746,     6,   167,     0, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100],\n",
      "        [ 3458,     5,     6,  4395,    17, 29163,     0, 58100, 58100, 58100,\n",
      "         58100, 58100],\n",
      "        [  481,    25, 34319,   916,     0, 58100, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100],\n",
      "        [  159,    24,    25, 18366,    34, 31334,   167,     0, 58100, 58100,\n",
      "         58100, 58100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "tensor([[   14,    17,  8281,   161,     7,    17,  8826,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [   14,    17, 13415,  1214,     7,  1262,   920,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2422,  7593,   526,    12,     4,  6424,   135,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  430,    17, 11514,   118,   223,     4,   736, 11962,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [32027,    82,    12,     4, 11408,  4595,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100]])\n"
     ]
    }
   ],
   "source": [
    "labels = tokenizer(labels, padding=True, truncation=True, return_tensors=\"pt\").input_ids\n",
    "features = tokenizer(features, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(features[:5])  \n",
    "print(labels[:5])                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features und labels auf GPU verschieben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features = features.to(device)\n",
    "labels = labels = labels.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features entsprechend des Models aufbereiten                                        Quelle: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_encodings, label_encodings):\n",
    "        self.input_encodings = input_encodings\n",
    "        self.label_encodings = label_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.input_encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.label_encodings[idx]\n",
    "        }\n",
    "\n",
    "train_data = TranslationDataset(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Model finetunen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikla\\AppData\\Local\\Temp\\ipykernel_22492\\2361781258.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  7%|▋         | 13/190 [00:00<00:09, 18.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8415, 'grad_norm': 64.23077392578125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 22/190 [00:01<00:08, 19.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6301, 'grad_norm': 67.9134292602539, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 33/190 [00:01<00:08, 19.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5764, 'grad_norm': 66.05239868164062, 'learning_rate': 3e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 42/190 [00:02<00:07, 19.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.1792, 'grad_norm': 56.003170013427734, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 52/190 [00:02<00:07, 19.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8176, 'grad_norm': 30.1253604888916, 'learning_rate': 5e-06, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 62/190 [00:03<00:06, 19.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5801, 'grad_norm': 8.995869636535645, 'learning_rate': 6e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 72/190 [00:03<00:05, 19.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2268, 'grad_norm': 7.555587291717529, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 83/190 [00:04<00:05, 19.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3151, 'grad_norm': 7.629572868347168, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 94/190 [00:04<00:04, 19.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9106, 'grad_norm': 6.773868083953857, 'learning_rate': 9e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 102/190 [00:05<00:04, 19.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7943, 'grad_norm': 6.327442646026611, 'learning_rate': 1e-05, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 112/190 [00:05<00:03, 19.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5931, 'grad_norm': 5.189174175262451, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 122/190 [00:06<00:03, 19.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5873, 'grad_norm': 5.166854381561279, 'learning_rate': 1.2e-05, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 132/190 [00:06<00:02, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4689, 'grad_norm': 5.7976789474487305, 'learning_rate': 1.3000000000000001e-05, 'epoch': 3.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 144/190 [00:07<00:02, 19.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.379, 'grad_norm': 5.3745245933532715, 'learning_rate': 1.4000000000000001e-05, 'epoch': 3.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 154/190 [00:08<00:01, 19.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3686, 'grad_norm': 5.704104423522949, 'learning_rate': 1.5e-05, 'epoch': 3.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 164/190 [00:08<00:01, 19.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2849, 'grad_norm': 6.007859706878662, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 173/190 [00:08<00:00, 19.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1823, 'grad_norm': 5.244335174560547, 'learning_rate': 1.7000000000000003e-05, 'epoch': 4.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 183/190 [00:09<00:00, 19.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1275, 'grad_norm': 5.185139179229736, 'learning_rate': 1.8e-05, 'epoch': 4.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:09<00:00, 19.67it/s]c:\\Users\\nikla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[58100]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1699, 'grad_norm': 8.596354484558105, 'learning_rate': 1.9e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 17.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10.8507, 'train_samples_per_second': 138.239, 'train_steps_per_second': 17.51, 'train_loss': 2.8438538902684263, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=190, training_loss=2.8438538902684263, metrics={'train_runtime': 10.8507, 'train_samples_per_second': 138.239, 'train_steps_per_second': 17.51, 'total_flos': 4766957568000.0, 'train_loss': 2.8438538902684263, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainingsargumente definieren\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Speichern der Ergebnisse\n",
    "    num_train_epochs=5,              # Anzahl der Epochen\n",
    "    per_device_train_batch_size=8,   # Batch-Größe\n",
    "    warmup_steps=500,                # Anzahl der Warmup-Schritte\n",
    "    weight_decay=0.01,               # Gewichtung für Regularisierung\n",
    "    logging_dir='./logs',            # Verzeichnis für Logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer definieren\n",
    "trainer = Trainer(\n",
    "    model=model,                         # Modell\n",
    "    args=training_args,                  # Trainingsargumente\n",
    "    train_dataset=train_data,            # Trainingsdatensatz\n",
    "    tokenizer=tokenizer                  # Tokenizer\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten zum testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_german_references = [\n",
    "    \"ein Fisch auf dem Trockenen\",\n",
    "    \"Öl ins Feuer gießen\",\n",
    "    \"bis zu den Zähnen bewaffnet sein\",\n",
    "    \"das Eis brechen\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdaten tokenisieren und Übersetzung anfertigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = tokenizer(test_german_references, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "test_inputs = test_inputs.to(device)\n",
    "\n",
    "# Modell generiert Übersetzungen\n",
    "generated_ids = model.generate(test_inputs[\"input_ids\"], max_length=128)\n",
    "\n",
    "# Decodieren der generierten IDs\n",
    "translations = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Übersetzung ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ein Fisch auf dem Trockenen\n",
      "Übersetzung: a fish on the rock\n",
      "\n",
      "Original: Öl ins Feuer gießen\n",
      "Übersetzung: water the fire\n",
      "\n",
      "Original: bis zu den Zähnen bewaffnet sein\n",
      "Übersetzung: be armed to the teth\n",
      "\n",
      "Original: das Eis brechen\n",
      "Übersetzung: break the ice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, translation in enumerate(translations):\n",
    "    print(f\"Original: {test_german_references[i]}\")\n",
    "    print(f\"Übersetzung: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_references = [\n",
    "    \"a fish out of water\",\n",
    "    \"add fuel to the fire\",\n",
    "    \"armed to the teeth\",\n",
    "    \"break the ice\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference sind die möglichen Übersetzungen \n",
    "weights=(0.5, 0.5, 0, 0)   hier wird festgelegt wie viele Wörter hintereinander richtig sein müssen. In dem Beispiel 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-Score für Satz 1: 0.32\n",
      "BLEU-Score für Satz 2: 0.30\n",
      "BLEU-Score für Satz 3: 0.55\n",
      "BLEU-Score für Satz 4: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\nikla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# BLEU-Score berechnen\n",
    "for i, translation in enumerate(translations):\n",
    "    reference = [english_references[i].split()]  # Tokenisierte Referenz\n",
    "    candidate = translation.split()  # Tokenisierte Vorhersage\n",
    "    bleu_score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "    print(f\"BLEU-Score für Satz {i + 1}: {bleu_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
